---
title: "DT 607---Fall 2019---Project 4"
author: "Team ADMJ"
date: "10/27/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r loadLibraries, message=FALSE}
library(tm)
library(data.table)
setDTthreads(0L)
```

# Assignment
It can be useful to be able to classify new "test" documents using already
classified "training" documents.  A common example is using a corpus of labeled
spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  

For this project, you can start with a spam/ham dataset, then predict the class
of new documents (either withheld from the training dataset or from another
source such as your own spam folder). One example corpus:
https://spamassassin.apache.org/old/publiccorpus/

# Solution
```{r Scratch}
Ham1 <- DirSource("./Data/easy_ham")
Spam <- DirSource("./Data/spam_2")
Ham1_c <- VCorpus(Ham1)
Spam_c <- VCorpus(Spam)
```

```{r}
transformations <- list(removePunctuation,
                        removeNumbers,
                        content_transformer(tolower))
Ham <- tm_map(Ham1_c, FUN = tm_reduce, tmFuns = transformations)
Ham <- tm_map(Ham, removeWords, stopwords("english"))
Ham <- tm_map(Ham, stripWhitespace)
Ham <- tm_map(Ham, stemDocument)

meta(Ham, tag = "class", type = "indexed") <- "ham"

Spam <- tm_map(Spam_c, stripWhitespace)
Spam <- tm_map(Spam, content_transformer(tolower))
Spam <- tm_map(Spam, removeNumbers)
Spam <- tm_map(Spam, removePunctuation)
Spam <- tm_map(Spam, )
Spam <- tm_map(Spam, stemDocument)
meta(Spam, tag = "class", type = "indexed") <- "spam"
```

```{r}

```