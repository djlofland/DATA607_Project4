---
title: "DT 607---Fall 2019---Project 4"
author: "Team ADMJ"
date: "11/1/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Assignment
It can be useful to be able to classify new "test" documents using already
classified "training" documents.  A common example is using a corpus of labeled
spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  

For this project, you can start with a spam/ham dataset, then predict the class
of new documents (either withheld from the training dataset or from another
source such as your own spam folder). One example corpus:
https://spamassassin.apache.org/old/publiccorpus/

# Solution
## Overview
### Executive Summary
The `tm` package will be used to create a corpus of data which will serve as the
source of features and observations for the analysis. This will then be
converted into a document-term matrix. Finally, The `caret` package will be used
for the model fitting, validation, and testing.

The process of building a ham/spam filter is an oft-used pedagogical tool when
teaching predictive modeling. Therefore, there is a multitude of information
available on-line and in texts, of which we availed ourselves.

It should be noted that one of the more common packages in recent use for text
mining, the `RTextTools` package was recently removed from CRAN, and personal
communication by one of us with the author (who is now building the news feed at 
[LinkedIn](https://www.linkedin.com/in/timjurka/)) confirmed that the package
is abandonware.

### Document-Term Matrix
A document-term matrix (DTM) is the model matrix used in natural language
processing (NLP). Its rows represent the documents in the corpus and its columns
represent the selected terms or tokens which are treated as features. The values
in each cell depends on the weighting schema selected. The simplest is
*term-frequency* (tf). This is just the number of times the word is found in
that document. A more sophisticated weighting scheme is *term frequencyâ€“inverse*
*document frequency* (tf-idf). This measure increases with the frequency of the
term, but offsets it by the number of documents in which it appears. This will
lower the predictive power of words that naturally appear very often in all
kinds of documents, and so do not shed much light on the type of document. This
problem is also addressed by removing words so common as to have no predictive
power at all like "and" or "the". These are often called *stop words*.

## Code and Process
### Style
In the following document, all user-created variables will be in `snake_case`
and all user-created functions will be in `CamelCase`. Unfortunately, the `tm`
packages uses `camelCase` for its functions. wE aPoLoGIze fOr anY IncoNVenIence.

### Load Libraries and Set Seed
```{r loadLibraries, message=FALSE}
set.seed(12)
library(doParallel)
cl <- makePSOCKcluster(6L)
registerDoParallel(cl)
library(tm)
library(SnowballC)
library(caret)
library(wordcloud)
```

### List files
The files were downloaded from the link above, and the `spam_2` and `easy_ham`
sets were selected for analysis. These were unzipped so that each email is its
own file in the directory.
```{r}
s_files <- list.files("./Data/spam_2", full.names = TRUE)
h_files <- list.files("./Data/easy_ham", full.names = TRUE)
h_len <- length(h_files)
s_len <- length(s_files)
```

## Building the Corpus
### Email Headers
We will be focusing on email content, and not the meta information or doing
reverse DNS lookups. Therefore, it makes sense to remove the email headers.
According to the most recent RFC about email,
[RFC 5322, Section 2.2](https://tools.ietf.org/html/rfc5322), the header should
not contain any purely blank lines. Therefore, it is a very reasonable approach
to look for the first blank line and only start ingesting the email from the
next line. That is what is searched for by the regex pattern `"^$"` in the
function below.

### Raw Corpus
The `readLines` function reads each line as a seperate vector. To turn this into
a single character vector, the `paste` function is used with the appropriate
`sep` and `collapse` values. The class of the document is passed as a parameter
to the `Build

```{r rawCorpus}
BuildCorpus <- function(files, class){
  for (i in seq_along(files)) {
    raw_text <- readLines(files[i])
    em_length <- length(raw_text)
    body_start <- min(grep("^$", raw_text, fixed = FALSE)) + 1L
    em_body <- paste(raw_text[body_start:em_length],
                     sep = "", collapse = " ")
    if (i == 1L) {
      ret_Corpus <- VCorpus(VectorSource(em_body))
    } else {
      tmp_Corpus <- VCorpus(VectorSource(em_body))
      ret_Corpus <- c(ret_Corpus, tmp_Corpus)
    }
  }
  meta(ret_Corpus, tag = "class", type = "indexed") <- class
  return(ret_Corpus)
}

h_corp_raw <- BuildCorpus(h_files, "ham")
s_corp_raw <- BuildCorpus(s_files, "spam")
```

### Cleaning the Corpus
We used many of the default cleaning tools in the `tm` package to peform
standard adjustments like lowercasing, removing numbers, etc. We made two
non-native adjustments. First we stripped out anything that looked like a URL.
This needed to be done prior to removing punctuation, of course. We also added a
few words to the removal list which we think have little predictive power due to
their overuse.

We considered removing all punctuation, but decided to leave intra-word
contractions.

Lastly, we used the `SnowballC` package to stem the document. This process
tries to identify common roots shared by similar words and then treat them as
one. For example:
```{r stem}
wordStem(c('run', 'running', 'ran', 'runt'), language = 'porter')
```

The complete cleaning rules are in the `CleanCorpus` function.
```{r}
# https://stackoverflow.com/questions/47410866/r-inspect-document-term-matrix-results-in-error-repeated-indices-currently-not
CleanCorpus <- function(corpus){
  overused_words <- c("ok", 'okay', 'day', "might", "bye", "hello", "hi",
                      "dear", "thank", "you", "please", "sorry")
  StripURL <- function(x) {gsub("(http[^ ]*)|(www\\.[^ ]*)", "", x)}
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, content_transformer(StripURL))
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, removePunctuation,
                   preserve_intra_word_contractions = TRUE,
                   preserve_intra_word_dashes = TRUE)
  corpus <- tm_map(corpus, removeWords, c(stopwords("english"), overused_words))
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, stemDocument)
  return(corpus)
}
```

### Removing Very Sparse Terms 
Even with a cleaned corpus, the overwhelming majority of the terms are rare.
There are two ways to address sparsity of terms in the `tm` package. The first
is to generate a list of words that appear at least \(k\) times in the corpus.
This is done using the `findFreqTerms` command. Then the document-term matrix
(DTM) can be built using only those words.

The second way is to build the DTM with all words, and then remove the words
that don't appear in at least \(p\%\) of documents. This is done using the
`removeSparseTerms` function in `tm`. Both methods make manual inspection of
more than one line of the matrix impossible. The matrix is stored sparsely as a
triplet, and once terms are removed, it becomes impossible for R to print
properly.

The `removeSparseTerms` is intuitively more appealing as it measures frequency
by document, and not across documents. However, applying that to three seperate
corpuses would result in the validation and testing sets not having the same
words as the training set. Therefore, the build-up method will be used, but used
by finding the remaining terms after calling remove.

However, before we do that, we need to discuss...

### Training, Validation, and Testing
Hastie & Tibshirani, in their seminal work ESL, suggest breaking ones data into
three parts: 50% training, 25% validation, and 25% testing. Confusingly, some
literature uses "test" for the validation set and "holdout" for the test set.
Regardless, the idea is that you train your model on 50% of the data, and use
25% of the data (the validation set) to refine any hyperparameters of the model.
You do this for each model, and then once all the models are tuned as best
possible, they are compared with each other by their performance on the
heretofore unused testing/holdout set. The `SplitSample` function was used to
split the data at the start.

However, the `caret` package has validation built-in to most of its model
selection and tuning procedures, (so we will break this data into two parts: 75%
for training/validation and 25% for testing (holdout).) OR DO WE VALIDATE ANYWAY

```{r, splitsets}
SplitSample <- function(n) {
  if (n %% 4 == 0) {
    n_split <- sample(c(rep("train", n / 2),
                      rep("validate", n / 4),
                      rep("test", n / 4)))
  } else {
    n_split <- sample(x = c("train", "validate", "test"), size = n,
                    replace = TRUE, prob = c(0.5, 0.25, 0.25))
  }
}
h_split <- SplitSample(h_len)
s_split <- SplitSample(s_len)

```

### Building the Term List
As both training and validation are part of the model construction, we feel that
the term list can be built from the combination of the two. The terms in the 
testing/holdout set will not be seen prior to testing. We will restrict the
word list to words that appear in at least 100 of the combined
`r (h_len + s_len) * 0.75` documents.
```{r toptermlist}
raw_train <- c(h_corp_raw[h_split == "train"],
               s_corp_raw[s_split == "train"])
raw_val <- c(h_corp_raw[h_split == "validate"],
             s_corp_raw[s_split == "validate"])
raw_test <- c(h_corp_raw[h_split == "test"],
              s_corp_raw[s_split == "train"])
raw_term_corp <- c(raw_train, raw_val)
clean_term_corp <- CleanCorpus(raw_term_corp)
dtm_terms <- DocumentTermMatrix(clean_term_corp, control = list(
  bounds = list(global = c(100L, Inf))))
freq_terms <- Terms(dtm_terms)
```

Here are the top 20 stemmed terms out of the `r length(freq_terms)` terms we will use
in the dictionary:
```{r ft}
ft <- colSums(as.matrix(dtm_terms))
ft_df <- data.frame(term = names(ft), count = as.integer(ft))
knitr::kable(head(ft_df[order(ft, decreasing = TRUE), ], n = 20L),
             row.names = FALSE)
```

Here is a histogram of word frequency using the
[Freedman-Diaconis](https://en.wikipedia.org/wiki/Freedman%E2%80%93Diaconis_rule)
rule for binwidth.
```{r freqhist}
bw_fd <- 2 * IQR(ft_df$count) / (dim(ft_df)[[1]]) ^ (1/3)
ggplot(ft_df, aes(x = count)) + geom_histogram(binwidth = bw_fd) + xlab("Term")
```

Finally, a wordcloud of the terms:
```{r ftwc}
wordcloud(ft_df$term,ft_df$count, scale = c(4, 0.6), min.freq = 100L,
          colors = brewer.pal(5, "Dark2"), random.color = TRUE,
          random.order = TRUE, rot.per = 0, fixed.asp = FALSE)
```

### Building the Training Set
```{r trainset}
# sample is to randomize the observations
clean_train <- sample(CleanCorpus(raw_train))
clean_train_type <- unlist(meta(clean_train, tag = "class"))
attributes(clean_train_type) <- NULL
clean_train_type <- as.factor(clean_train_type)
dtm_train <- DocumentTermMatrix(clean_train,
                                control = list(dictionary = freq_terms))
dtm_train
```

Compare the above with the sparsity of the cleaned training corpus without the
limiting dictionary:
```{r dtmcomp}
dtm_train_S <- DocumentTermMatrix(clean_train)
dtm_train_S
```

### Building the Validation Set
```{r valset}
clean_val <- sample(CleanCorpus(raw_val))
clean_val_type <- unlist(meta(clean_val, tag = "class"))
attributes(clean_val_type) <- NULL
clean_val_type <- as.factor(clean_val_type)
dtm_val <- DocumentTermMatrix(clean_val,
                              control = list(dictionary = freq_terms))
```

### Building the Testing Set
```{r testset}
clean_test <- sample(CleanCorpus(raw_test))
clean_test_type <- unlist(meta(clean_test, tag = "class"))
attributes(clean_test_type) <- NULL
clean_test_type <- as.factor(clean_test_type)
dtm_test <- DocumentTermMatrix(clean_test,
                              control = list(dictionary = freq_terms))
```

### Last step
All the classification algorithms work much better if the values are factors and
not numbers. So we will convert them as we create the model matrices.
```{r modelprep}
CreateModelMatrix <- function(dtm) {
  apply(as.matrix(dtm),
        MARGIN = 2,
        FUN = function(x) factor(ifelse(x > 0, 1L, 0L),
                                 levels = c(0L, 1L),
                                 labels = c("Not Found", "Found")))
  }
train_sp <- CreateModelMatrix(dtm_train)
val_sp <- CreateModelMatrix(dtm_val)
test_sp <- CreateModelMatrix(dtm_test)
```

## Train Models
### Overview
Now we can train the models. The process will generally follow the following
path:

  1. Select a model family (logistic regression, random forest, etc.)
  1. Use the `caret` package on the training set to pick "best" model givem the
  supplied control, preprocessing, or other [hyper-]parameters. This may include
  some level of validation
  1. Switch the hyperparameters, train again, and compare using validation set
  1. Select "best" model from family
  1. Repeat with other families
  1. Compare performance of final selections using testing/holdout set
  1. Take a well-deserved vacation
  
As the `caret` package serves as an umbrella for over 230 model types living in
different packages, we may select a less-sophisticated version of a family if it
reduces code complexity and migraine propensity. Forgive us as well if we don't
explain every family and every selection. Below we create the model matrices
which will be passed to `caret`.


### Logistic Regression
```{r logistic, message=FALSE, warning=FALSE, error=FALSE, cache=2L}
# 10-fold CV
tr_ctrl <- trainControl(method = "cv", number = 10L, classProbs = TRUE,
                        summaryFunction = twoClassSummary)
LogR1 <- train(x = train_sp, y = clean_train_type, method = "glm",
              family = "binomial", trControl = tr_ctrl, metric = "ROC")
LogR1
LogR1v <- predict(LogR1, val_sp)
confusionMatrix(LogR1v, clean_val_type)

# 5 iterations of 5-fold CV
tr_ctrl <- trainControl(method = "repeatedcv", number = 5L, repeats = 5L,
                        classProbs = TRUE, summaryFunction = twoClassSummary)
LogR2 <- train(x = train_sp, y = clean_train_type, method = "glm",
              family = "binomial", trControl = tr_ctrl, metric = "ROC")
LogR2
LogR2v <- predict(LogR2, val_sp)
confusionMatrix(LogR2v, clean_val_type)

# Monte-Carlo Cross validation using 75/25 and 10 iterations
tr_ctrl <- trainControl(method = "LGOCV", number = 5L, p = 0.75,
                        classProbs = TRUE, summaryFunction = twoClassSummary)
LogR3 <- train(x = train_sp, y = clean_train_type, method = "glm",
              family = "binomial", trControl = tr_ctrl, metric = "ROC")
LogR3
LogR3v <- predict(LogR3, val_sp)
confusionMatrix(LogR3v, clean_val_type)

```

As all of the validation sets had the same performance, our selected model will
be the simplest.

### Naive Bayes
```{r naiveb, message=FALSE, warning=FALSE, error=FALSE, cache=2L}
# 10-fold CV
tr_ctrl <- trainControl(method = "cv", number = 10L, classProbs = TRUE,
                        summaryFunction = twoClassSummary)
NB1 <- train(x = train_sp, y = clean_train_type, method = "nb",
             tuneGrid = expand.grid(fL = c(0, 0.5, 1),
                                   usekernel = c(TRUE, FALSE),
                                   adjust = c(0, 0.5, 1)),
             trControl = tr_ctrl, metric = "ROC")
NB1
NB1v <- predict(NB1, val_sp)
confusionMatrix(NB1v, clean_val_type)

# Monte-Carlo Cross validation using 2:1 and 10 iterations
tr_ctrl <- trainControl(method = "LGOCV", number = 5L, p = 0.75,
                        classProbs = TRUE, summaryFunction = twoClassSummary)
NB2 <- train(x = train_sp, y = clean_train_type, method = "nb",
             tuneGrid = expand.grid(fL = c(0, 0.5, 1),
                                   usekernel = c(TRUE, FALSE),
                                   adjust = c(0, 0.5, 1)),
             trControl = tr_ctrl, metric = "ROC")
NB2
NB2v <- predict(NB2, val_sp)
confusionMatrix(NB2v, clean_val_type)
```

# Epilogue
```{stopParallel}
sessionInfo()
stopCluster(cl)
```