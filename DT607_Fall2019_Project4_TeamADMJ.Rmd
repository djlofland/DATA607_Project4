---
title: "DT 607---Fall 2019---Project 4"
author: "Team ADMJ"
date: "11/1/2019"
output:
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Assignment
It can be useful to be able to classify new "test" documents using already
classified "training" documents.  A common example is using a corpus of labeled
spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  

For this project, you can start with a spam/ham data-set, then predict the class
of new documents (either withheld from the training data-set or from another
source such as your own spam folder). One example corpus:
https://spamassassin.apache.org/old/publiccorpus/

# Solution
## Overview
### Executive Summary
The `tm` package will be used to create a corpus of data which will serve as the
source of features and observations for the analysis. This will then be
converted into a document-term matrix. Finally, The `caret` package will be used
for the model fitting, validation, and testing.

The process of building a ham/spam filter is an oft-used pedagogical tool when
teaching predictive modeling. Therefore, there is a multitude of information
available on-line and in texts, of which we availed ourselves.

It should be noted that one of the more common packages in recent use for text
mining, the `RTextTools` package was recently removed from CRAN, and personal
communication by one of us with the author (who is now building the news feed at 
[LinkedIn](https://www.linkedin.com/in/timjurka/)) confirmed that the package
is abandonware.

Lastly, we understand that the object of this exercise is not to build an
excellent predictor but to demonstrate the necessary knowledge required to build
classification algorithms.

### Document-Term Matrix
A document-term matrix (DTM) is the model matrix used in natural language
processing (NLP). Its rows represent the documents in the corpus and its columns
represent the selected terms or tokens which are treated as features. The values
in each cell depends on the weighting schema selected. The simplest is
*term-frequency* (tf). This is just the number of times the word is found in
that document. A more sophisticated weighting scheme is *term frequencyâ€“inverse*
*document frequency* (tf-idf). This measure increases with the frequency of the
term, but offsets it by the number of documents in which it appears. This will
lower the predictive power of words that naturally appear very often in all
kinds of documents, and so do not shed much light on the type of document. This
problem is also addressed by removing words so common as to have no predictive
power at all like "and" or "the". These are often called *stop words*.

## Code and Process
### Style
In the following document, all user-created variables will be in `snake_case`
and all user-created functions will be in `CamelCase`. Unfortunately, the `tm`
packages uses `camelCase` for its functions. wE aPoLoGIze fOr anY IncoNVenIence.

### Load Libraries and Set Seed
```{r loadLibraries, message=FALSE}
set.seed(12)
library(doParallel)
cl <- makePSOCKcluster(6L)
registerDoParallel(cl)
library(tm)
library(SnowballC)
library(caret)
library(wordcloud)
```

### List files
The files were downloaded from the link above, and the `spam_2` and `easy_ham`
sets were selected for analysis. These were unzipped so that each email is its
own file in the directory.
```{r listFiles}
s_files <- list.files("./Data/spam_2", full.names = TRUE)
h_files <- list.files("./Data/easy_ham", full.names = TRUE)
h_len <- length(h_files)
s_len <- length(s_files)
```

## Building the Corpus
### Email Headers
We will be focusing on email content, and not the meta information or doing
reverse DNS lookups. Therefore, it makes sense to remove the email headers.
According to the most recent RFC about email,
[RFC 5322, Section 2.2](https://tools.ietf.org/html/rfc5322), the header should
not contain any purely blank lines. Therefore, it is a very reasonable approach
to look for the first blank line and only start ingesting the email from the
next line. That is what is searched for by the regex pattern `"^$"` in the
function below.

### Raw Corpus
The `readLines` function reads each line as a separate vector. To turn this into
a single character vector, the `paste` function is used with the appropriate
`sep` and `collapse` values. The class of the document is passed as a parameter
to the `Build

```{r rawCorpus}
BuildCorpus <- function(files, class){
  for (i in seq_along(files)) {
    raw_text <- readLines(files[i])
    em_length <- length(raw_text)
    body_start <- min(grep("^$", raw_text, fixed = FALSE)) + 1L
    em_body <- paste(raw_text[body_start:em_length],
                     sep = "", collapse = " ")
    if (i == 1L) {
      ret_Corpus <- VCorpus(VectorSource(em_body))
    } else {
      tmp_Corpus <- VCorpus(VectorSource(em_body))
      ret_Corpus <- c(ret_Corpus, tmp_Corpus)
    }
  }
  meta(ret_Corpus, tag = "class", type = "indexed") <- class
  return(ret_Corpus)
}

h_corp_raw <- BuildCorpus(h_files, "ham")
s_corp_raw <- BuildCorpus(s_files, "spam")
```

### Cleaning the Corpus
We used many of the default cleaning tools in the `tm` package to perform
standard adjustments like lower-casing, removing numbers, etc. We made two
non-native adjustments. First we stripped out anything that looked like a URL.
This needed to be done prior to removing punctuation, of course. We also added a
few words to the removal list which we think have little predictive power due to
their overuse. We considered removing all punctuation, but decided to leave both
intra-word contractions and internal punctuation.

Lastly, we used the `SnowballC` package to stem the document. This process
tries to identify common roots shared by similar words and then treat them as
one. For example:
```{r stem}
wordStem(c('run', 'running', 'ran', 'runt'), language = 'porter')
```

The complete cleaning rules are in the `CleanCorpus` function.
```{r cleanCorpus}
# https://stackoverflow.com/questions/47410866/r-inspect-document-term-matrix-results-in-error-repeated-indices-currently-not
CleanCorpus <- function(corpus){
  overused_words <- c("ok", 'okay', 'day', "might", "bye", "hello", "hi",
                      "dear", "thank", "you", "please", "sorry")
  StripURL <- function(x) {gsub("(http[^ ]*)|(www\\.[^ ]*)", "", x)}
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, content_transformer(StripURL))
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, removePunctuation,
                   preserve_intra_word_contractions = TRUE,
                   preserve_intra_word_dashes = TRUE)
  corpus <- tm_map(corpus, removeWords, c(stopwords("english"), overused_words))
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, stemDocument)
  return(corpus)
}
```

### Removing Very Sparse Terms 
Even with a cleaned corpus, the overwhelming majority of the terms are rare.
There are two ways to address sparsity of terms in the `tm` package. The first
is to generate a list of words that appear at least \(k\) times in the corpus.
This is done using the `findFreqTerms` command. Then the document-term matrix
(DTM) can be built using only those words.

The second way is to build the DTM with all words, and then remove the words
that don't appear in at least \(p\%\) of documents. This is done using the
`removeSparseTerms` function in `tm`. Both methods make manual inspection of
more than one line of the matrix impossible. The matrix is stored sparsely as a
triplet, and once terms are removed, it becomes impossible for R to print
properly.

The `removeSparseTerms` is intuitively more appealing as it measures frequency
by document, and not across documents. However, applying that to three separate
corpuses would result in the validation and testing sets not having the same
words as the training set. Therefore, the build-up method will be used, but used
by finding the remaining terms after calling remove.

However, before we do that, we need to discuss...

### Training, Validation, and Testing
Hastie & Tibshirani, in their seminal work ESL, suggest breaking ones data into
three parts: 50% training, 25% validation, and 25% testing. Confusingly, some
literature uses "test" for the validation set and "holdout" for the test set.
Regardless, the idea is that you train your model on 50% of the data, and use
25% of the data (the validation set) to refine any hyper-parameters of the model.
You do this for each model, and then once all the models are tuned as best
possible, they are compared with each other by their performance on the
heretofore unused testing/holdout set. The `SplitSample` function was used to
split the data at the start.
```{r splitsets}
SplitSample <- function(n) {
  if (n %% 4 == 0) {
    n_split <- sample(c(rep("train", n / 2),
                      rep("validate", n / 4),
                      rep("test", n / 4)))
  } else {
    n_split <- sample(x = c("train", "validate", "test"), size = n,
                    replace = TRUE, prob = c(0.5, 0.25, 0.25))
  }
}
h_split <- SplitSample(h_len)
s_split <- SplitSample(s_len)

```

### Building the Term List
As both training and validation are part of the model construction, we feel that
the term list can be built from the combination of the two. The terms in the 
testing/holdout set will not be seen prior to testing. We will restrict the
word list to words that appear in at least 100 of the combined
`r (h_len + s_len) * 0.75` documents.
```{r toptermlist}
raw_train <- c(h_corp_raw[h_split == "train"],
               s_corp_raw[s_split == "train"])
raw_val <- c(h_corp_raw[h_split == "validate"],
             s_corp_raw[s_split == "validate"])
raw_test <- c(h_corp_raw[h_split == "test"],
              s_corp_raw[s_split == "train"])
raw_term_corp <- c(raw_train, raw_val)
clean_term_corp <- CleanCorpus(raw_term_corp)
dtm_terms <- DocumentTermMatrix(clean_term_corp, control = list(
  bounds = list(global = c(100L, Inf))))
freq_terms <- Terms(dtm_terms)
```

Here are the top 20 stemmed terms out of the `r length(freq_terms)` terms we will use
in the dictionary:
```{r freqTerms}
ft <- colSums(as.matrix(dtm_terms))
ft_df <- data.frame(term = names(ft), count = as.integer(ft))
knitr::kable(head(ft_df[order(ft, decreasing = TRUE), ], n = 20L),
             row.names = FALSE)
```

Here is a histogram of word frequency using the
[Freedman-Diaconis](https://en.wikipedia.org/wiki/Freedman%E2%80%93Diaconis_rule)
rule for binwidth.
```{r freqhist}
bw_fd <- 2 * IQR(ft_df$count) / (dim(ft_df)[[1]]) ^ (1/3)
ggplot(ft_df, aes(x = count)) + geom_histogram(binwidth = bw_fd) + xlab("Term")
```

Finally, a wordcloud of the stemmed terms appearing at least 250 times:
```{r ftwc, message=FALSE, warning=FALSE, error=FALSE}
wordcloud(ft_df$term,ft_df$count, scale = c(3, 0.6), min.freq = 250L,
          colors = brewer.pal(5, "Dark2"), random.color = TRUE,
          random.order = TRUE, rot.per = 0, fixed.asp = FALSE)
```

### Building the Training Set
```{r trainset}
# sample is to randomize the observations
clean_train <- sample(CleanCorpus(raw_train))
clean_train_type <- unlist(meta(clean_train, tag = "class"))
attributes(clean_train_type) <- NULL
dtm_train <- DocumentTermMatrix(clean_train,
                                control = list(dictionary = freq_terms))
dtm_train
```

Compare the above with the sparsity of the cleaned training corpus without the
limiting dictionary:
```{r dtmcomp}
dtm_train_S <- DocumentTermMatrix(clean_train)
dtm_train_S
```

### Building the Validation Set
```{r valset}
clean_val <- sample(CleanCorpus(raw_val))
clean_val_type <- unlist(meta(clean_val, tag = "class"))
attributes(clean_val_type) <- NULL
dtm_val <- DocumentTermMatrix(clean_val,
                              control = list(dictionary = freq_terms))
```

### Building the Testing Set
```{r testset}
clean_test <- sample(CleanCorpus(raw_test))
clean_test_type <- unlist(meta(clean_test, tag = "class"))
attributes(clean_test_type) <- NULL
dtm_test <- DocumentTermMatrix(clean_test,
                              control = list(dictionary = freq_terms))
```

### Last step
The `caret` package requires its input to be a numeric matrix. As the DTM is a
special form of sparse matrix, we need to convert it to something `caret`
understands. The response vector must be a factor for classification, which is
why all three `clean_x_type` vectors were created as factors.
```{r modelprep}
train_m <- as.matrix(dtm_train)
clean_train_type <- factor(clean_train_type, levels = c("spam", "ham"))
val_m <- as.matrix(dtm_val)
clean_val_type <- factor(clean_val_type, levels = c("spam", "ham"))
test_m <- as.matrix(dtm_test)
clean_test_type <- factor(clean_test_type, levels = c("spam", "ham"))
```

## Train Models
### Overview
Now we can train the models. The process will generally follow the following
path:

  1. Select a model family (logistic regression, random forest, etc.)
  1. Use the `caret` package on the training set to pick "best" model given the
  supplied control, pre-processing, or other [hyper-]parameters. This may include
  some level of validation
  1. Switch the hyper-parameters, train again, and compare using validation set
  1. Select "best" model from family
  1. Repeat with other families
  1. Compare performance of final selections using testing/holdout set
  1. Take a well-deserved vacation
  
As the `caret` package serves as an umbrella for over 230 model types living in
different packages, we may select a less-sophisticated version of a family if it
reduces code complexity and migraine propensity. Forgive us as well if we don't
explain every family and every selection. Below we create the model matrices
which will be passed to `caret`.

Experimentation was done with many of the tuning parameters. However, most
increases in accuracy came at an inordinate expense of time. Therefore, for the
purposes of this exercise, many of the more advantageous options will be
limited. For example, cross-validation will be limited to single-pass ten-fold.
In production, one should be more vigorous, of course.

### Optimization Metric
Usually, AUC, a function of ROC, is used for classification problems. However,
for imbalanced data sets it is suggested to use one of precision, recall, or F1
instead. See
[here](https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9),
[here](https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c),
or [here](https://acutecaretesting.org/en/articles/precision-recall-curves-what-are-they-and-how-are-they-used)
for examples.

In our case, the data set is imbalanced, and the cost of a false positive
(classifying ham as spam) is greater than a false negative. Originally, we
selected precision as the metric, as hitting the "junk" button for something in
your inbox is less annoying than having your boss's email sit in your junk
folder.

However, as we trained models, we found some fascinating results. In one of the
random forest models, the algorithm found a better model with one less false
positive, at the expense of 61 more false negatives. Therefore, we decided to
redo the tests using the balanced F1 as the optimization metric.

### Logistic Regression
This is the classic good-old logistic regression in R. There are no hyper/tuning
parameters, so the only comparison can be between the method of
cross-validation.
```{r logistic, message=FALSE, warning=FALSE, error=FALSE, cache=2L}
# 10-fold CV
tr_ctrl <- trainControl(method = "cv", number = 10L, classProbs = TRUE,
                        summaryFunction = prSummary)
LogR1 <- train(x = train_m, y = clean_train_type, method = "glm",
              family = "binomial", trControl = tr_ctrl, metric = "F")
LogR1
LogR1v <- predict(LogR1, val_m)
confusionMatrix(LogR1v, clean_val_type, mode = "prec_recall", positive = "spam")

# Monte-Carlo Cross validation using 75/25 and 5 iterations
tr_ctrl <- trainControl(method = "LGOCV", number = 10L, p = 0.75,
                        classProbs = TRUE, summaryFunction = prSummary)
LogR2 <- train(x = train_m, y = clean_train_type, method = "glm",
              family = "binomial", trControl = tr_ctrl, metric = "F")
LogR2
LogR2v <- predict(LogR2, val_m)
confusionMatrix(LogR2v, clean_val_type, mode = "prec_recall", positive = "spam")
```

Both versions performed the same on the validation set. As the first has a
slightly better F-score, we will select that one.

### Random Forest
The `ranger` package is used as the random forest engine due to its being
optimized for higher dimensions.
```{r rfA, message=FALSE, warning=FALSE, error=FALSE, cache=2L}
tr_ctrl <- trainControl(method = "cv", number = 10L, classProbs = TRUE,
                        summaryFunction = prSummary)
RF1 <- train(x = train_m, y = clean_train_type, method = 'ranger',
             trControl = tr_ctrl, metric = "F", tuneLength = 5L)
RF1
RF1v <- predict(RF1, val_m)
confusionMatrix(RF1v, clean_val_type, mode = "prec_recall", positive = "spam")
```

Let's do a bit wider search among tuning parameters.
```{r rfB, message=FALSE, warning=FALSE, error=FALSE, cache=2L}
rf_grid <- expand.grid(mtry = seq(8, 48, 4),
                       splitrule = c('gini', 'extratrees'),
                       min.node.size = c(1L, 10L))
RF2 <- train(x = train_m, y = clean_train_type, method = 'ranger',
             trControl = tr_ctrl, metric = "F", tuneGrid = rf_grid)
RF2
RF2v <- predict(RF2, val_m)
confusionMatrix(RF2v, clean_val_type, mode = "prec_recall", positive = "spam")
```

### Naive Bayes
```{r nbA, message=FALSE, warning=FALSE, error=FALSE, cache=2L}
tr_ctrl <- trainControl(method = "cv", number = 10L, classProbs = TRUE,
                        summaryFunction = prSummary)
NB1 <- train(x = train_m, y = clean_train_type, method = "nb",
             trControl = tr_ctrl, metric = "F")
NB1
NB1v <- predict(NB1, val_m)
confusionMatrix(NB1v, clean_val_type, mode = "prec_recall", positive = "spam")
```

This is an *awfully* performing model. Naive Bayes is known to be very sensitive
to class imbalances. Let's implement up-sampling and a wider search. 
```{r nbB, message=FALSE, warning=FALSE, error=FALSE, cache=2L}
tr_ctrl <- trainControl(method = "cv", number = 10L, classProbs = TRUE,
                        summaryFunction = prSummary, sampling = 'up')
nb_grid <- expand.grid(usekernel = TRUE,
                       fL = seq(0.25, 0.75, 0.05),
                       adjust = 1)
NB2 <- train(x = train_m, y = clean_train_type, method = "nb",
             trControl = tr_ctrl, metric = "F", tuneGrid = nb_grid)
NB2
NB2v <- predict(NB2, val_m)
confusionMatrix(NB2v, clean_val_type, mode = "prec_recall", positive = "spam")
```

Results are still **miserable**. Maybe this wasn't used properly.

### Neural Network
```{r NN1, message=FALSE, warning=FALSE, error=FALSE, cache=2L}
tr_ctrl <- trainControl(method = "cv", number = 10L, classProbs = TRUE,
                        summaryFunction = prSummary)
NN1 <- train(x = train_m, y = clean_train_type, method = "nnet", trace = FALSE,
             trControl = tr_ctrl, metric = "F", tuneLength = 5L, maxit = 250L)
NN1
NN1v <- predict(NN1, val_m)
confusionMatrix(NN1v, clean_val_type, mode = "prec_recall", positive = "spam")
```

Some light tuning:

```{r NN2, message=FALSE, warning=FALSE, error=FALSE, cache=2L}
nn_grid <- expand.grid(size = 1L, decay = c(0.99, seq(0.95, 0.05, -0.05), 0.01))
NN2 <- train(x = train_m, y = clean_train_type, method = "nnet", trace = FALSE,
             trControl = tr_ctrl, metric = "F", tuneGrid = nn_grid,
             maxit = 250L)
NN2
NN2v <- predict(NN2, val_m)
confusionMatrix(NN2v, clean_val_type, mode = "prec_recall", positive = "spam")
```

### Gradient Boosted Machines
```{r GBM1, message=FALSE, warning=FALSE, error=FALSE, cache=2L}
tr_ctrl <- trainControl(method = "cv", number = 10L, classProbs = TRUE,
                        summaryFunction = prSummary)
GBM1 <- train(x = train_m, y = clean_train_type, method = "gbm", verbose = FALSE,
              trControl = tr_ctrl, tuneLength = 5L, metric = "F")
GBM1v <- predict(GBM1, val_m)
confusionMatrix(GBM1v, clean_val_type, mode = "prec_recall", positive = "spam")
```

This model looks really good. Let's throw a little extra fine-tuning in. After
running a wide-scale grid, the best option is selected below, so that the entire
grid doesn't have to rerun every time.
```{r GBM2, message=FALSE, warning=FALSE, error=FALSE, cache=2L}
gbm_grid <- expand.grid(n.trees = 400L,
                     interaction.depth = 7L,
                     shrinkage = 0.1,
                     n.minobsinnode = 10L)
GBM2 <- train(x = train_m, y = clean_train_type, method = "gbm", verbose = FALSE,
              trControl = tr_ctrl, tuneGrid = gbm_grid, metric = "F")
GBM2
GBM2v <- predict(GBM2, val_m)
confusionMatrix(GBM2v, clean_val_type, mode = "prec_recall", positive = "spam")
```

### Other models
With over 230 possible models, there are many more options to train, like
XGBoost, Neural Networks, Bayesian Regression, Support Vector Machines, etc.

## Test Models
The best models in the above categories will now be compared against the
testing/holdout set:
```{r test, message=FALSE, warning=FALSE, error=FALSE, cache=2L}
LogRt <- predict(LogR1, test_m)
RFt <- predict(RF1, test_m)
NNt <- predict(NN1, test_m)
NBt <- predict(NB2, test_m) # For laughs
GBMt <- predict(GBM2, test_m)
confusionMatrix(LogRt, clean_test_type, mode = "prec_recall", positive = "spam")
confusionMatrix(RFt, clean_test_type, mode = "prec_recall", positive = "spam")
confusionMatrix(NNt, clean_test_type, mode = "prec_recall", positive = "spam")
confusionMatrix(NBt, clean_test_type, mode = "prec_recall", positive = "spam")
confusionMatrix(GBMt, clean_test_type, mode = "prec_recall", positive = "spam")
```

From these models, while the logistic had no false negatives---a recall of 1---it
did so by coding 57 good emails as spam. The remaining models all did quite well,
but the winner is the random forest model, with the highest F-score and fewest
miscategorized emails of any type.

# Epilogue
```{r epilogue}
sessionInfo()
stopCluster(cl)
```